{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**I. Ingestion des commentaires de HESPRESS à l'aide du web scraping et de leur stockage dans MongoDB :**"
      ],
      "metadata": {
        "id": "hVaQOnXGEZsk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from pymongo import MongoClient\n",
        "\n",
        "# Define the URL to scrape\n",
        "url=\"https://www.hespress.com/%d8%a3%d8%b7%d8%b1-%d8%a7%d9%84%d8%a3%d9%83%d8%a7%d8%af%d9%8a%d9%85%d9%8a%d8%a9-%d9%8a%d8%ad%d8%aa%d8%ac%d9%88%d9%86-%d8%a8%d9%85%d8%af%d9%8a%d8%b1%d9%8a%d8%a9-%d8%a7%d9%84%d9%81%d9%82%d9%8a%d9%87-1116934.html\"\n",
        "# Make a GET request to the website\n",
        "response = requests.get(url)\n",
        "\n",
        "# Parse the HTML using BeautifulSoup\n",
        "soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "# Extract the comments from the HTML\n",
        "comments = soup.find_all(\"div\", class_=\"comments\")\n",
        "\n",
        "# Connect to the MongoDB instance\n",
        "client = MongoClient(\"mongodb+srv://asma:1234@cluster0.lbpc9ip.mongodb.net/?retryWrites=true&w=majority5\")\n",
        "\n",
        "# Get the comments collection\n",
        "comments_collection = client.test.comments\n",
        "\n",
        "comments_to_insert = []\n",
        "# Iterate through the comments and extract the text of the two div elements\n",
        "for comment in comments:\n",
        "    for ul in comment.find_all('ul'):\n",
        "        for li in ul.find_all('li'):\n",
        "            div_container = li.find('div', {'class': 'comment-body'})\n",
        "            div1 = div_container.find('div', {'class': 'comment-head'})\n",
        "            div2 = div_container.find('div', {'class': 'comment-text'})\n",
        "            comment_obj = {\"author\": div1.text.split(\"\\n\")[2], \"text\": div2.text}\n",
        "            comments_to_insert.append(comment_obj)\n",
        "\n",
        "# insert all comments\n",
        "if len(comments_to_insert) > 0:\n",
        "    comments_collection.insert_many(comments_to_insert)"
      ],
      "metadata": {
        "id": "CJDanLKiGT8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install kafka-python"
      ],
      "metadata": {
        "id": "VmVfDoMx3P90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install kafka\n"
      ],
      "metadata": {
        "id": "zawhihti9IHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install confluent_kafka\n"
      ],
      "metadata": {
        "id": "X4Zt_J0OHyXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from confluent_kafka import Producer, Consumer\n",
        "import json\n",
        "\n",
        "# Kafka producer\n",
        "producer = Producer({'bootstrap.servers': 'localhost:9092'})\n",
        "\n",
        "# Iterate through the comments and extract the text of the two div elements\n",
        "for comment in comments:\n",
        "    for ul in comment.find_all('ul'):\n",
        "        for li in ul.find_all('li'):\n",
        "            div_container = li.find('div', {'class': 'comment-body'})\n",
        "            div1 = div_container.find('div', {'class': 'comment-head'})\n",
        "            div2 = div_container.find('div', {'class': 'comment-text'})\n",
        "            comment_obj = {\"author\": div1.text.split(\"\\n\")[2], \"text\": div2.text}\n",
        "            comment_json = json.dumps(comment_obj)\n",
        "            producer.produce('exam', value=bytes(comment_json, 'utf-8'))\n",
        "            producer.flush()\n",
        "            print(f'Published message: {comment_json}')\n",
        "\n",
        "\n",
        "# Kafka consumer\n",
        "conf = {'bootstrap.servers': 'localhost:9092',\n",
        "        'group.id': 'mygroup',\n",
        "        'auto.offset.reset': 'earliest'}\n",
        "\n",
        "consumer = Consumer(conf)\n",
        "\n",
        "consumer.subscribe(['exam'])\n",
        "\n",
        "\n",
        "\n",
        "while True:\n",
        "    msg = consumer.poll(1.0)\n",
        "    if msg is None:\n",
        "        continue\n",
        "    if msg.error():\n",
        "        if msg.error().code() == KafkaError._PARTITION_EOF:\n",
        "            print('Reached end of topic {} [{}] at offset {}'.format(\n",
        "                msg.topic(), msg.partition(), msg.offset()))\n",
        "        else:\n",
        "            print('Error occured: {}'.format(msg.error()))\n",
        "    else:\n",
        "        comment = json.loads(msg.value())\n",
        "        print(comment)\n",
        "consumer.close()"
      ],
      "metadata": {
        "id": "J7l49NJ1xSjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I. Data ingestion into a Kafka topic. The data is in the form of comments scraped from a web page and is ingested into a Kafka topic named \"exam\".**texte en gras**"
      ],
      "metadata": {
        "id": "DlKQfH5tldt5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**II. Apache Kafka is used to stream the ingested data in real-time for processing and analysis. This code is similar to the previous script, but with an added functionality of sentiment analysis using the 'NLTK' library.**\n"
      ],
      "metadata": {
        "id": "Y500S9QXlZeO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from confluent_kafka import Producer, Consumer\n",
        "\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# download the necessary NLTK resources\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Create a SentimentIntensityAnalyzer object\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Iterate through the comments and extract the text of the two div elements\n",
        "for comment in comments:\n",
        "    for ul in comment.find_all('ul'):\n",
        "        for li in ul.find_all('li'):\n",
        "            div_container = li.find('div', {'class': 'comment-body'})\n",
        "            div1 = div_container.find('div', {'class': 'comment-head'})\n",
        "            div2 = div_container.find('div', {'class': 'comment-text'})\n",
        "            comment_obj = {\"author\": div1.text, \"text\": div2.text}\n",
        "            comment_text = comment_obj[\"text\"]\n",
        "            sentiment = sia.polarity_scores(comment_text)\n",
        "            print(\"Comment text: \", comment_text)\n",
        "            print(\"Sentiment: \", sentiment)\n",
        "# Kafka consumer\n",
        "conf = {'bootstrap.servers': 'localhost:9092',\n",
        "        'group.id': 'mygroup',\n",
        "        'auto.offset.reset': 'earliest'}\n",
        "\n",
        "consumer = Consumer(conf)\n",
        "\n",
        "consumer.subscribe(['exam'])\n",
        "\n",
        "while True:\n",
        "    msg = consumer.poll(1.0)\n",
        "    if msg is None:\n",
        "        continue\n",
        "    if msg.error():\n",
        "        if msg.error().code() == KafkaError._PARTITION_EOF:\n",
        "            print('Reached end of topic {} [{}] at offset {}'.format(\n",
        "                msg.topic(), msg.partition(), msg.offset()))\n",
        "        else:\n",
        "            print('Error occured: {}'.format(msg.error()))\n",
        "    else:\n",
        "        comment = json.loads(msg.value())\n",
        "        print(comment)\n",
        "# close the consumer after the loop\n",
        "consumer.close()"
      ],
      "metadata": {
        "id": "bT8qBxShlPi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**II. Apache Kafka is used to stream the ingested data in real-time for processing and analysis. This code is similar to the previous script, but with an added functionality of sentiment analysis using the 'TextBlob' library.**"
      ],
      "metadata": {
        "id": "1BNvT3_olohh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "for comment in comments:\n",
        "    for ul in comment.find_all('ul'):\n",
        "        for li in ul.find_all('li'):\n",
        "            div_container = li.find('div', {'class': 'comment-body'})\n",
        "            div2 = div_container.find('div', {'class': 'comment-text'})\n",
        "            comment_text = div2.text\n",
        "            blob = TextBlob(comment_text)\n",
        "            sentiment = blob.sentiment\n",
        "            print(sentiment)\n",
        "while True:\n",
        "    msg = consumer.poll(1.0)\n",
        "    if msg is None:\n",
        "        continue\n",
        "    if msg.error():\n",
        "        if msg.error().code() == KafkaError._PARTITION_EOF:\n",
        "            print('Reached end of topic {} [{}] at offset {}'.format(\n",
        "                msg.topic(), msg.partition(), msg.offset()))\n",
        "        else:\n",
        "            print('Error occured: {}'.format(msg.error()))\n",
        "    else:\n",
        "        comment = json.loads(msg.value())\n",
        "        print(comment)\n",
        "# close the consumer after the loop\n",
        "consumer.close()"
      ],
      "metadata": {
        "id": "vQHPttG8lnuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**III.This code sets environment variables for Apache Spark.**"
      ],
      "metadata": {
        "id": "2mRNSRp6l9CP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
        "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
      ],
      "metadata": {
        "id": "gmRU-yIhl1LA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**III. Spark is used to perform batch processing on the data by grouping the comments by author and converting the values into a list**"
      ],
      "metadata": {
        "id": "dzplaRefmFNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder.appName(\"BatchProcessing\").getOrCreate()\n",
        "\n",
        "# Load data from the local JSON file into a DataFrame\n",
        "df = spark.read.option(\"multiline\",\"true\").json(r\"C:\\Users\\21263\\Downloads\\comments.json\")\n",
        "\n",
        "# Perform batch processing on the data\n",
        "result = df.rdd\\\n",
        "    .map(lambda x: (x.author, x.text))\\\n",
        "    .groupByKey()\\\n",
        "    .mapValues(list)\\\n",
        "    .collect()\n",
        "\n",
        "# Show result\n",
        "for author, text in result:\n",
        "    print(f\"Author: {author}\\nText: {text}\\n\")\n",
        "\n",
        "# Stop Spark session\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "uKgIF6XVmDBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**III. Spark is used to perform batch processing on the data by applying sentiment analysis using the library 'TextBlob' :**"
      ],
      "metadata": {
        "id": "AFP0XNfXmLP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder.appName(\"BatchProcessing\").getOrCreate()\n",
        "\n",
        "# Load data from the local JSON file into a DataFrame\n",
        "df = spark.read.option(\"multiline\",\"true\").json(r\"C:\\Users\\21263\\Downloads\\comments.json\")\n",
        "\n",
        "# Perform batch processing on the data\n",
        "result = df.rdd\\\n",
        "    .map(lambda x: (x.author, x.text))\\\n",
        "    .groupByKey()\\\n",
        "    .mapValues(list)\\\n",
        "    .collect()\n",
        "\n",
        "# Perform sentiment analysis using TextBlob\n",
        "for author, text in result:\n",
        "    sentiment_scores = [TextBlob(t).sentiment.polarity for t in text]\n",
        "    average_sentiment = sum(sentiment_scores) / len(sentiment_scores)\n",
        "    if average_sentiment != 0:\n",
        "        print(f\"Author: {author}\\nAverage sentiment: {average_sentiment}\\n\")\n",
        "\n",
        "# Stop Spark session\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "rqUNe3vOmLd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**III. Spark is used to perform batch processing on the data by showing the most commonly used words:**"
      ],
      "metadata": {
        "id": "6gtFFOlMmaKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import add\n",
        "import re\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import split, explode, desc\n",
        "\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder.appName(\"BatchProcessing\").getOrCreate()\n",
        "\n",
        "# Load data from the local JSON file into a DataFrame\n",
        "df = spark.read.option(\"multiline\",\"true\").json(r\"C:\\Users\\21263\\Downloads\\comments.json\")\n",
        "\n",
        "# Split the \"text\" field into individual words\n",
        "df = df.select(\"author\", explode(split(df[\"text\"], \" \")).alias(\"word\"))\n",
        "\n",
        "# Group by word and count occurrences\n",
        "word_counts = df.groupBy(\"word\").count().sort(desc(\"count\"))\n",
        "\n",
        "# Show the most commonly used words\n",
        "word_counts.show(10)\n",
        "\n",
        "# Stop Spark session\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "tabuQmRkmYKf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}